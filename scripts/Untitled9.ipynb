{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7c6175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchmetrics import ConfusionMatrix\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "from operator import itemgetter\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from google.colab import files\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acc2387",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrna_df = pd.read_csv('/content/drive/MyDrive/Data/scRNA_gb2_inputs_labels.tsv', sep='\\t')\n",
    "scrna_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f58349",
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_df = pd.read_csv('/content/drive/MyDrive/Data/Lambda_values.csv')\n",
    "sup_df.drop(0, inplace = True)\n",
    "sup_df.reset_index(drop = True, inplace = True)\n",
    "sup_df.rename(columns={'cell id':'cell'}, inplace = True)\n",
    "sup_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ec486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subpops(df):\n",
    "    my_passed = dict()\n",
    "    sub_pops = list()\n",
    "\n",
    "    i = 0\n",
    "    for x in df['Cell specific barcode']:\n",
    "        if x not in my_passed:\n",
    "            sub_pops.append(i)\n",
    "            my_passed[x] = i\n",
    "            i += 1\n",
    "        else:\n",
    "            sub_pops.append(my_passed[x])\n",
    "    return sub_pops\n",
    "\n",
    "scrna_df['Sub-Populations'] = create_subpops(sup_df)\n",
    "print(scrna_df['Sub-Populations'].max())\n",
    "scrna_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a53a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesenchymal_proneural = pd.read_csv('/content/drive/MyDrive/Data/mesenchymal_proneural.csv')\n",
    "mesenchymal_proneural = mesenchymal_proneural.rename(columns={\"Unnamed: 0\": \"cell\"})\n",
    "mesenchymal_proneural.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1862cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrna_df['Mesenchymal'] = mesenchymal_proneural['Mesenchymal']\n",
    "scrna_df['Proneural'] = mesenchymal_proneural['Proneural']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data(df):\n",
    "    cell_type = list()\n",
    "    var_df = np.var(df.iloc[:,2:-1])\n",
    "    ind = np.argpartition(var_df.values, -3000)[-3000:]\n",
    "    X = df.iloc[:, ind]\n",
    "    cols = list(X.columns)\n",
    "\n",
    "    samples = X.to_numpy()\n",
    "    clusters = df['cluster']\n",
    "    subpops = df['Sub-Populations']\n",
    "    proneural = df['Proneural']\n",
    "    mesenchymal = df['Mesenchymal']\n",
    "\n",
    "    for x in df['cell']:\n",
    "        if x.endswith('_1'):\n",
    "            cell_type.append('Control')\n",
    "        else:\n",
    "            cell_type.append('Treatment')\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        all_data.append((torch.FloatTensor(samples[i]), clusters[i], subpops[i], proneural[i], mesenchymal[i], cell_type[i]))\n",
    "\n",
    "    all_df = pd.DataFrame()\n",
    "    all_df['Data'] = all_data\n",
    "    all_df['Clusters'] = clusters\n",
    "    all_df['Sub_pops'] = subpops\n",
    "    all_df['Cell_type'] = cell_type\n",
    "    all_df['Proneural'] = proneural\n",
    "    all_df['Mesenchymal'] = mesenchymal\n",
    "\n",
    "    all_df = all_df.sample(frac = 1)\n",
    "    train_inter, test_df = train_test_split(all_df, stratify = clusters, test_size = 0.15)\n",
    "    train_df, valid_df = train_test_split(train_inter, stratify = train_inter['Clusters'], test_size = 0.1)\n",
    "\n",
    "    for temp_df in [all_df, train_df, test_df, valid_df]:\n",
    "        temp_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    print(\"Length of training df:\", len(train_df))\n",
    "    print(\"Length of testing df:\", len(test_df))\n",
    "    print(\"Length of validation df:\", len(valid_df))\n",
    "\n",
    "    return cols, all_df, train_df, test_df, valid_df\n",
    "\n",
    "cols, all_df, train_df, test_df, valid_df = combine_data(scrna_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c01bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing that the ratio of classes is maintained\n",
    "y = scrna_df['cluster']\n",
    "print(y.value_counts().sort_index() / len(y))\n",
    "print(train_df['Clusters'].value_counts().sort_index() / len(train_df))\n",
    "print(test_df['Clusters'].value_counts().sort_index() / len(test_df))\n",
    "print(valid_df['Clusters'].value_counts().sort_index() / len(valid_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13002345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_iterator(data_list, BATCH_SIZE = 64):\n",
    "    iterator = data.DataLoader(data_list,\n",
    "                            shuffle=True,\n",
    "                            batch_size = BATCH_SIZE)\n",
    "    return iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2d3033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def biggest_subpops(df, p):\n",
    "    percent_pop = df['Sub_pops'].value_counts().sort_index() / len(df['Sub_pops'])\n",
    "    max_counts = percent_pop[percent_pop > p]\n",
    "\n",
    "    max_count_list = list(max_counts.index)\n",
    "\n",
    "    #print(max_counts)\n",
    "\n",
    "    #print(len(max_count_list))\n",
    "\n",
    "    values = [i for i in range(len(max_count_list))]\n",
    "    count_subpops = dict(zip(max_count_list, values))\n",
    "\n",
    "    temp_df = df[df['Sub_pops'].isin(max_count_list)]\n",
    "    final_df = temp_df.copy()\n",
    "\n",
    "    reordered_subpops = list()\n",
    "    new_subpop_tuple = list()\n",
    "    \n",
    "    for x in final_df['Data']:\n",
    "        reordered_subpops.append(count_subpops[x[2]])\n",
    "        new_subpop_tuple.append((x[0], x[1], count_subpops[x[2]], x[3], x[4], x[5]))\n",
    "\n",
    "    final_df['Subpopulations'] = reordered_subpops\n",
    "    final_df['Data Max Subpops'] = new_subpop_tuple\n",
    "    final_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    return final_df, count_subpops\n",
    "\n",
    "# p = 0.01\n",
    "all_subpops, count_subpops = biggest_subpops(all_df, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9132763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subpop_in_cluster(df):\n",
    "    dict1 = dict()\n",
    "    for i, my_cluster in enumerate(df['Clusters']):\n",
    "        x = df['Subpopulations'][i]\n",
    "        if my_cluster not in dict1:\n",
    "            dict1[my_cluster] = dict()\n",
    "        if x not in dict1[my_cluster]:\n",
    "            dict1[my_cluster][x] = 1\n",
    "        else:\n",
    "            dict1[my_cluster][x] += 1\n",
    "    return dict1\n",
    "\n",
    "def all_iterators(df, p):\n",
    "    list_df = list()\n",
    "    tuple_data = list(df['Data'])\n",
    "    all_iterator = create_iterator(tuple_data)\n",
    "\n",
    "    max_subpops_df, _ = biggest_subpops(df, p)\n",
    "    list_df.append(max_subpops_df)\n",
    "    shortened_iterator = create_iterator(list(max_subpops_df['Data Max Subpops']))\n",
    "    all_pie = subpop_in_cluster(max_subpops_df)\n",
    "\n",
    "    type_iterators = list()\n",
    "    type_pies = list()\n",
    "    for cell_type in ['Control', 'Treatment']:\n",
    "        split_data = max_subpops_df[max_subpops_df['Cell_type'] == cell_type]\n",
    "        split_data.reset_index(inplace = True, drop = True)\n",
    "        list_df.append(split_data)\n",
    "        type_pies.append(subpop_in_cluster(split_data))\n",
    "        type_iterators.append(create_iterator(list(split_data['Data Max Subpops'])))\n",
    "\n",
    "    return list_df, all_pie, type_pies[0], type_pies[1], all_iterator, shortened_iterator, type_iterators[0], type_iterators[1]\n",
    "\n",
    "#all_pie, control_pie, treatment_pie, all_iterator, shortened_iterator, control_iterator, treatment_iterator = all_iterators(all_df, 0.01)\n",
    "\n",
    "#all_iterator = iterator with all data\n",
    "#shortened_iterator = iterator that has cells with subpops that make up at least p of the total number of cells\n",
    "#type_iterators[0] = control_iterator = iterator with control cells in largest subpops\n",
    "#type_iterators[1] = treatment_iterator = iterator with treatment cells in largest subpops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfde6750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_box_plot(df, my_type, title):\n",
    "    df1 = df.groupby('Subpopulations', as_index=False)[my_type].median().sort_values(by = my_type, ascending = False)\n",
    "    df[my_type + ' Real Values'] = df[my_type]\n",
    "    ww = df1.merge(df, on = 'Subpopulations')\n",
    "    #df2 = df1.reindex(df1.median().sort_values().index, axis=1)\n",
    "    fig = px.box(df1, x=\"Subpopulations\", y=my_type, title = title)\n",
    "    #go.Figure([go.Box(y=df2[c], name=meta[c][\"name\"], line={\"color\":meta[c][\"color\"]}) for c in df2.columns])\n",
    "\n",
    "make_box_plot(list_df[0], 'Proneural', 'All')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a93e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_subpop(df):\n",
    "    fig = make_subplots(rows=2, cols=7, shared_yaxes = True, shared_xaxes=True)\n",
    "    row_num = 1\n",
    "    col_num = 1\n",
    "    for x in range(15):\n",
    "        if col_num == 8:\n",
    "            row_num = 2\n",
    "            col_num = 1\n",
    "        temp_df = df[df[\"Subpopulations\"] == x]\n",
    "        fig.add_trace(go.Scatter(x=temp_df['Mesenchymal'], y=temp_df['Proneural'], mode=\"markers\"), row=row_num, col=col_num)\n",
    "        col_num += 1\n",
    "    return fig\n",
    "\n",
    "new_fig = plot_subpop(list_df[2])\n",
    "new_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c94d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = ['blue', 'red', 'green', 'purple', 'orange', 'pink', 'grey', 'blue', 'red', 'green', 'purple', 'orange', 'pink', 'grey']\n",
    "\n",
    "def centeroidnp(arr):\n",
    "    length = arr.shape[0]\n",
    "    sum_x = np.sum(arr[:, 0])\n",
    "    sum_y = np.sum(arr[:, 1])\n",
    "    return sum_x/length, sum_y/length\n",
    "\n",
    "def matplot_subpop(df):\n",
    "    clf = NearestCentroid()\n",
    "    centroid_coords = list()\n",
    "    plt.figure(figsize=(40,40))\n",
    "    #subpops = df[\"Subpopulations\"].unique().sort()\n",
    "    for subpop in range(14):\n",
    "        temp_df = df[df[\"Subpopulations\"] == subpop]\n",
    "        my_x = list(temp_df['Mesenchymal'])\n",
    "        my_y = list(temp_df['Proneural'])\n",
    "        my_zip = zip(my_x, my_y)\n",
    "        X = np.array([i for i in my_zip])\n",
    "        centroid_coord = centeroidnp(X)\n",
    "        centroid_coords.append(centroid_coord)\n",
    "        plt.subplot(4,4,subpop+1)\n",
    "        plt.xlim([0.05, 0.2])\n",
    "        plt.ylim([0.05, 0.2])\n",
    "        plt.scatter(my_x, my_y, c = l1[subpop])\n",
    "        plt.scatter(centroid_coord[0], centroid_coord[1], c = 'black')\n",
    "        plt.title('Subpopulation ' + str(subpop))\n",
    "    plt.show()\n",
    "    return centroid_coords\n",
    "\n",
    "def vector_subplot(control, treatment):\n",
    "    plt.figure(figsize=(40,40))\n",
    "    for i in range(14):\n",
    "        plt.subplot(4,4,i+1)\n",
    "        plt.quiver(control[i][0], control[i][1], treatment[i][0], treatment[i][1], color=l1, units='xy', scale=1)\n",
    "        plt.xlim([0, 0.5])\n",
    "        plt.ylim([0, 0.5])\n",
    "        plt.title('Subpopulation ' + str(i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e9bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histo_graph(df):\n",
    "    skews = list()\n",
    "    plt.figure(figsize=(40,40))\n",
    "    subpops = df['Subpopulations'].unique()\n",
    "    for i,subpop in enumerate(subpops):\n",
    "        temp_df = df[df['Subpopulations'] == subpop]\n",
    "        plt.subplot(5,3,i+1)\n",
    "        y = temp_df['Mesenchymal'].values\n",
    "        plt.hist(y, bins='auto')\n",
    "        plt.title('Subpopulation ' + str(i) + ', Skew = ' + str(np.round(skew(y),3)))\n",
    "        skews.append(skew(y))\n",
    "        plt.show()\n",
    "    return skews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e4b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making pie plots\n",
    "\n",
    "def no_split(my_dict):\n",
    "    not_split_pie = dict()\n",
    "    for cluster in my_dict:\n",
    "        for subpop in my_dict[cluster]:\n",
    "            if subpop not in not_split_pie:\n",
    "                not_split_pie[subpop] = my_dict[cluster][subpop]\n",
    "            else:\n",
    "                not_split_pie[subpop] += my_dict[cluster][subpop]\n",
    "    return not_split_pie\n",
    "\n",
    "def make_split_pie(i, pie_data):\n",
    "    df = pd.DataFrame()\n",
    "    df['Subpopulations'] = [*pie_data[i].keys()]\n",
    "    df['Num'] = [*pie_data[i].values()]\n",
    "    fig = px.pie(df, values = 'Num', names = 'Subpopulations')\n",
    "    #fig.update_traces(textposition='inside')\n",
    "    return fig, df\n",
    "\n",
    "def make_combined_pie(pie_data):\n",
    "    df = pd.DataFrame()\n",
    "    df['Subpopulations'] = [*pie_data.keys()]\n",
    "    df['Num'] = [*pie_data.values()]\n",
    "    fig = px.pie(df, values = 'Num', names = 'Subpopulations')\n",
    "    #fig.update_traces(textposition='inside')\n",
    "    return fig, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7af37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subpop_change(df_control, df_treatment):\n",
    "    merged_df = df_control.merge(df_treatment, on='Subpopulations')\n",
    "    merged_df['Control'] = merged_df['Num_x']/sum(merged_df['Num_x'])\n",
    "    merged_df['Treatment'] = merged_df['Num_y']/sum(merged_df['Num_y'])\n",
    "    df2 = pd.DataFrame()\n",
    "    df2['Subpops'] = 2*list(merged_df['Subpopulations'])\n",
    "    df2['Cell Type'] = len(merged_df)*['Control'] + len(merged_df)*['Treatment']\n",
    "    df2['Percent'] = list(merged_df['Control']) + list(merged_df['Treatment'])\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5611e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizontal_bar_graph(df, df_type):\n",
    "    perc = list()\n",
    "    for x in df['Num']:\n",
    "        perc.append(x/sum(df['Num']))\n",
    "        df['Percent'] = perc\n",
    "        df[\"Subpopulations\"] = df[\"Subpopulations\"].astype(str)\n",
    "        df.sort_values(by=['Percent'], inplace = True)\n",
    "        df['Type'] = len(df)*[df_type]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0e46d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an alluvial plot\n",
    "\n",
    "def alluvial_change(pie_df_control, pie_df_treatment):\n",
    "    merged_df = pie_df_control.merge(treatment_pie_df, on='Subpopulations')\n",
    "    merged_df['Control'] = merged_df['Num_x']/sum(merged_df['Num_x'])\n",
    "    merged_df['Treatment'] = merged_df['Num_y']/sum(merged_df['Num_y'])\n",
    "\n",
    "    control = pd.DataFrame()\n",
    "    control['Subpops'] = list(merged_df['Subpopulations'])\n",
    "    control['Cell Type'] = len(merged_df)*['Control']\n",
    "    control['Percent'] = list(merged_df['Control'])\n",
    "\n",
    "    treatment = pd.DataFrame()\n",
    "    treatment['Subpops'] = list(merged_df['Subpopulations'])\n",
    "    treatment['Cell Type'] = len(merged_df)*['Treatment']\n",
    "    treatment['Percent'] = list(merged_df['Treatment'])\n",
    "\n",
    "    extra = list()\n",
    "    for x in control['Subpops']:\n",
    "        extra.append(' ')\n",
    "  \n",
    "    labels = 2*list(control['Subpops'])\n",
    "    node_colors = len(labels)*[f'rgba(255,0,255,{0.7})']\n",
    "    x_position = len(control['Subpops'])*[0.1]\n",
    "\n",
    "    y_position = list()\n",
    "    for i in range(len(control['Subpops'])):\n",
    "        y_position.append(0.1*i+0.1)\n",
    "        y_position = y_position*3\n",
    "\n",
    "    x_position += len(control['Subpops'])*[0.5]\n",
    "    labels += extra\n",
    "    x_position += len(extra)*[0.25]\n",
    "    node_colors += len(extra)*[f'rgba(255,0,255,{0})']\n",
    "\n",
    "    values = list()\n",
    "    indx1 = list()\n",
    "    indx2 = list()\n",
    "    link_color = list()\n",
    "    my_len = len(labels)\n",
    "    for i, c in enumerate(control['Percent']):\n",
    "        t = treatment['Percent'][i]\n",
    "        if t < c:\n",
    "            values.append(t)\n",
    "            indx1.append(i)\n",
    "            indx2.append(i + len(control['Subpops']))\n",
    "            link_color.append(f'rgba(255,0,255,{0.7})')\n",
    "            values.append(c - t)\n",
    "            indx1.append(i)\n",
    "            indx2.append(i + 2*len(control['Subpops']))\n",
    "            link_color.append(f'rgba(255,0,255,{0.1})')\n",
    "        if c < t:\n",
    "            values.append(c)\n",
    "            indx1.append(i)\n",
    "            indx2.append(i + len(control['Subpops']))\n",
    "            link_color.append(f'rgba(255,0,255,{0.7})')\n",
    "            values.append(t - c)\n",
    "            indx1.append(i + 2*len(control['Subpops']))\n",
    "            indx2.append(i + len(control['Subpops']))\n",
    "            link_color.append(f'rgba(255,0,255,{0.1})')\n",
    "\n",
    "    fig = go.Figure(go.Sankey(\n",
    "        node = dict(\n",
    "            pad = 15,\n",
    "            thickness = 10,\n",
    "            x = x_position,\n",
    "            y = y_position,\n",
    "            line = dict(color = \"black\", width = 0),\n",
    "            label = labels,\n",
    "            color = node_colors\n",
    "        ),\n",
    "        link = dict(\n",
    "            source = indx1,\n",
    "            target = indx2,\n",
    "            value = values,\n",
    "            color = link_color\n",
    "        )))\n",
    "  \n",
    "    fig.update_layout(title_text=\"My Sankey Diagram\", font_size=10)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a1e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_fc = nn.Linear(input_dim, 1000)\n",
    "        self.hidden_1 = nn.Linear(1000, 300)\n",
    "        self.drop = nn.Dropout(p = 0.2)\n",
    "       # self.batch_norm = nn.BatchNorm1d(300, affine=False)\n",
    "        self.hidden_2 = nn.Linear(300, 50)\n",
    "        self.output_fc = nn.Linear(50, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x = [batch size, height, width]\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        # x = [batch size, height * width]\n",
    "\n",
    "        h = torch.tanh(self.input_fc(x))\n",
    "\n",
    "        h_1 = torch.tanh(self.hidden_1(h))\n",
    "\n",
    "        h_2 = self.drop(h_1)\n",
    "\n",
    "        #h_3 = self.batch_norm(h_2)\n",
    "\n",
    "        h_3 = torch.tanh(self.hidden_2(h_2))\n",
    "\n",
    "        y_pred = self.output_fc(h_3)\n",
    "\n",
    "        #y_pred = [batch size, output dim]\n",
    "\n",
    "        return y_pred, h_3\n",
    "\n",
    "INPUT_DIM = 3000\n",
    "OUTPUT_DIM = 15\n",
    "\n",
    "model = MLP(INPUT_DIM, OUTPUT_DIM)\n",
    "\n",
    "#explainer = shap.DeepExplainer(model, x_train)\n",
    "#shap_values = explainer.shap_values(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141ed2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae36fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e6e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9489c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    top_pred = y_pred.argmax(1, keepdim=True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a7d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for (x, y, _, _, _, _) in tqdm(iterator, desc=\"Training\", leave=False):\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred, _ = model(x)\n",
    "\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe76d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    labels = []\n",
    "    probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for (x, y, _, _, _, _) in tqdm(iterator, desc=\"Evaluating\", leave=False):\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred, h = model(x)\n",
    "\n",
    "            y_prob = F.softmax(y_pred, dim=-1)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "            labels.append(y.cpu())\n",
    "            probs.append(y_prob.cpu())\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    probs = torch.cat(probs, dim=0)\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), labels, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1db64c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d82e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(train_iterator, valid_iterator, test_iterator):\n",
    "    EPOCHS = 10\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    history = {'Train': {'Accuracy': [], 'Loss': []}, 'Test': {'Accuracy': [], 'Loss': []}, 'Validation': {'Accuracy': [], 'Loss': []}}\n",
    "\n",
    "    for epoch in trange(EPOCHS):\n",
    "\n",
    "        start_time = time.monotonic()\n",
    "\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)\n",
    "        valid_loss, valid_acc, labels, probs = evaluate(model, valid_iterator, criterion, device)\n",
    "        history['Train']['Loss'].append(train_loss)\n",
    "        history['Train']['Accuracy'].append(train_acc)\n",
    "        history['Validation']['Loss'].append(valid_loss)\n",
    "        history['Validation']['Accuracy'].append(valid_acc)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "\n",
    "        end_time = time.monotonic()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "    test_loss, test_acc, _, _ = evaluate(model, test_iterator, criterion, device)\n",
    "\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c58d34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_individual_subpops(mlp_output, mlp_inter, subpop_dict, sub_pops):\n",
    "    tsne = manifold.TSNE(n_components=2)\n",
    "    tsne_out = tsne.fit_transform(mlp_output)\n",
    "    tsne_inter = tsne.fit_transform(mlp_inter)\n",
    "\n",
    "    out_subpops = dict()\n",
    "    inter_subpops = dict()\n",
    "    for i in [*subpop_dict.values()]:\n",
    "        x = (sub_pops == i)\n",
    "        out_subpops[i] = tsne_out[x]\n",
    "        inter_subpops[i] = tsne_inter[x]\n",
    "  \n",
    "  return tsne_out, tsne_inter, out_subpops, inter_subpops\n",
    "\n",
    "def plot_subpops_individual(my_dict):\n",
    "    plt.figure(figsize=(40,40))\n",
    "    for i in range(len(my_dict)):\n",
    "        my_x = my_dict[i][:, 0]\n",
    "        my_y = my_dict[i][:, 1]\n",
    "        plt.subplot(5,4,i+1)\n",
    "        plt.scatter(my_x, my_y)\n",
    "        plt.title('Subpopulation ' + str(i))\n",
    "    plt.show()\n",
    "\n",
    "def sihloutte(i, my_dict):\n",
    "    for k in range(2, 10):\n",
    "        model = KMeans(n_clusters=k)\n",
    "        model.fit(my_dict[i])\n",
    "        pred = model.predict(my_dict[i])\n",
    "        score = silhouette_score(my_dict[i], pred)\n",
    "        print('Silhouette Score for k = {}: {:<.3f}'.format(k, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680f8ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.DeepExplainer(model, x_train)\n",
    "shap_values = explainer.shap_values(x_test)\n",
    "shap.decision_plot(explainer.expected_value[0], shap_values[0][0], features = cols, feature_names = cols)\n",
    "shap.plots._waterfall.waterfall_legacy(explainer.expected_value[0], shap_values[0][0], feature_names = cols)\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0][0], features = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d267ffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturbe_single_genes(list_important_cols, train_data3, valid_data3, test_data3):\n",
    "    single_accuracies = list()\n",
    "\n",
    "    for x in list_important_cols:\n",
    "        indx1 = cols.index(x)\n",
    "\n",
    "        C = np.delete(train_data3, indx1, axis=1)\n",
    "        print(C.shape)\n",
    "        print(train_data3.shape)\n",
    "        W = np.split(C, len(C))\n",
    "        Z = zip(W, list(train_df['Clusters']))\n",
    "\n",
    "        D = np.delete(valid_data3, indx1, axis=1)\n",
    "        W1 = np.split(D, len(D))\n",
    "        Z1 = zip(W1, list(valid_df['Clusters']))\n",
    "\n",
    "        E = np.delete(test_data3,[indx1, axis=1)\n",
    "        W2 = np.split(E, len(E))\n",
    "        Z2 = zip(W2, list(test_df['Clusters']))\n",
    "\n",
    "        new_train_iterator = create_iterator(list(Z))\n",
    "        new_valid_iterator = create_iterator(list(Z1))\n",
    "        new_test_iterator = create_iterator(list(Z2))\n",
    "        single_accuracies.append(run(new_train_iterator, new_valid_iterator, new_test_iterator))\n",
    "    return single_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c4980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gene_pairs(pairs, accuracies):\n",
    "    pred_actuals = pd.DataFrame([(genes, acc) for genes, acc in zip(pairs, accuracies)], columns=['Gene Pairs', 'Accuracies'])\n",
    "    pred_actuals[['Gene1','Gene2']] = pd.DataFrame(pred_actuals['Gene Pairs'].tolist(), index= pred_actuals.index)\n",
    "    pred_actuals['Pairs'] = pred_actuals[['Gene1', 'Gene2']].apply(lambda x: ', '.join(x), axis=1)\n",
    "    pred_actuals['|Difference in Accuracy|'] = (pred_actuals['Accuracies'] - 0.81).abs()\n",
    "    fd = pred_actuals.sort_values(by='|Difference in Accuracy|', ascending = True)\n",
    "    fig = px.bar(fd, x=\"|Difference in Accuracy|\", y='Pairs', orientation='h')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5557d990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturbe_gene_pairs(list_important_cols, train_data3, valid_data3, test_data3):\n",
    "\n",
    "    accuracies = list()\n",
    "    pairs = list()\n",
    "\n",
    "    for i,x in enumerate(list_important_cols):\n",
    "        for x2 in list_imp_cols[1+i:]:\n",
    "            pairs.append((x,x2))\n",
    "            indx1 = cols.index(x)\n",
    "            indx2 = cols.index(x2)\n",
    "\n",
    "            C = np.delete(train_data3, [indx1, indx2], axis=1)\n",
    "            print(C.shape)\n",
    "            print(train_data3.shape)\n",
    "            W = np.split(C, len(C))\n",
    "            Z = zip(W, list(train_df['Clusters']))\n",
    "\n",
    "            D = np.delete(valid_data3, [indx1, indx2], axis=1)\n",
    "            W1 = np.split(D, len(D))\n",
    "            Z1 = zip(W1, list(valid_df['Clusters']))\n",
    "\n",
    "            E = np.delete(test_data3, [indx1, indx2], axis=1)\n",
    "            W2 = np.split(E, len(E))\n",
    "            Z2 = zip(W2, list(test_df['Clusters']))\n",
    "\n",
    "            new_train_iterator = create_iterator(list(Z))\n",
    "            new_valid_iterator = create_iterator(list(Z1))\n",
    "            new_test_iterator = create_iterator(list(Z2))\n",
    "            accuracies.append(run(new_train_iterator, new_valid_iterator, new_test_iterator))\n",
    "            plot_gene_pairs(pairs, accuracies)\n",
    "            \n",
    "    return pairs, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11242f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pertube_gene_triples():\n",
    "    triple_accuracies = list()\n",
    "    triples = list()\n",
    "\n",
    "    for i,x in enumerate(list_imp_cols):\n",
    "        for j, x2 in list_imp_cols[1+i:]:\n",
    "            for x3 in list_imp_cols[1+j]:\n",
    "                triples.append((x,x2))\n",
    "                indx1 = cols.index(x)\n",
    "                indx2 = cols.index(x2)\n",
    "\n",
    "                C = np.delete(train_data3, [indx1, indx2], axis=1)\n",
    "                print(C.shape)\n",
    "                print(train_data3.shape)\n",
    "                W = np.split(C, len(C))\n",
    "                Z = zip(W, list(train_df['Clusters']))\n",
    "\n",
    "                D = np.delete(valid_data3, [indx1, indx2], axis=1)\n",
    "                W1 = np.split(D, len(D))\n",
    "                Z1 = zip(W1, list(valid_df['Clusters']))\n",
    "\n",
    "                E = np.delete(test_data3, [indx1, indx2], axis=1)\n",
    "                W2 = np.split(E, len(E))\n",
    "                Z2 = zip(W2, list(test_df['Clusters']))\n",
    "\n",
    "                new_train_iterator = create_iterator(list(Z))\n",
    "                new_valid_iterator = create_iterator(list(Z1))\n",
    "                new_test_iterator = create_iterator(list(Z2))\n",
    "                accuracies.append(run(new_train_iterator, new_valid_iterator, new_test_iterator))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
